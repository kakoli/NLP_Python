{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "python-text-mining",
      "graded_item_id": "2qbcK",
      "launcher_item_id": "pi9Sh",
      "part_id": "kQiwX"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "TopicModelling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0mm1_pcQ4fl",
        "colab_type": "text"
      },
      "source": [
        "#### Topic Modelling\n",
        "Using Gensim's LDA model (Latent Dirichlet Allocation) to model topics in `newsgroup_data`, extract 10 topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEYNwrWQJLot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "77bad5b9-98be-4980-d890-4210c2cafc93"
      },
      "source": [
        "import pickle\n",
        "import gensim\n",
        "from google.colab import drive\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "dir=\"/content/gdrive/My Drive/Colab Notebooks/NLP/\"\n",
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqjDEtlRQ4gF",
        "colab_type": "code",
        "outputId": "dc942984-a54c-4f89-98c1-cbfb8b72f103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# Load the list of documents\n",
        "with open(dir + 'newsgroups', 'rb') as f: # rb - read in binary mode\n",
        "  # byte stream converted to python object, list of len 2000\n",
        "  newsgroup_data = pickle.load(f)\n",
        "\n",
        "# Use CountVectorizor to find three letter tokens, remove stop_words, \n",
        "# remove tokens that don't appear in at least 20 documents, appear in more than 20% of the documents\n",
        "vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english',\n",
        "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
        "# Fit and transform - X size (2000, 901)\n",
        "X = vect.fit_transform(newsgroup_data)\n",
        "\n",
        "# Convert sparse matrix to streaming gensim corpus (gensim.matutils.Sparse2Corpus)\n",
        "# sklearn describes documents as rows\n",
        "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
        "\n",
        "# vect.vocabulary_.items() returns {word:id} (To be used in LdaModel's id2word parameter)\n",
        "id_word_map = dict((id, word) for word, id in vect.vocabulary_.items())\n",
        "print(\"Vocab size\", len(id_word_map))\n",
        "{key: id_word_map[key] for key in list(id_word_map)[:10]}"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size 901\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{23: 'address',\n",
              " 33: 'america',\n",
              " 76: 'best',\n",
              " 335: 'group',\n",
              " 409: 'know',\n",
              " 514: 'new',\n",
              " 544: 'organization',\n",
              " 726: 'similar',\n",
              " 842: 'usa',\n",
              " 899: 'york'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqMwWUWGQ4gI",
        "colab_type": "code",
        "outputId": "58e302e5-e63e-497d-ba00-b6cb20a4b892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create LDA model on the corpus\n",
        "\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=id_word_map, passes=25, random_state=34)\n",
        "print(gensim.__version__)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ19drcTQ4gL",
        "colab_type": "text"
      },
      "source": [
        "#### lda_topics\n",
        "\n",
        "*Returns list of 10 tuples (topic_id, string of prob and topmost words)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02w6taYgQ4gM",
        "colab_type": "code",
        "outputId": "fe27c969-ce48-4352-f2df-9c14e1cae838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "def lda_topics():\n",
        "    topic_list = ldamodel.show_topics(num_topics=10, num_words=10)    \n",
        "    return topic_list\n",
        "\n",
        "lda_topics()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.056*\"edu\" + 0.043*\"com\" + 0.033*\"thanks\" + 0.022*\"mail\" + 0.021*\"know\" + 0.020*\"does\" + 0.014*\"info\" + 0.012*\"monitor\" + 0.010*\"looking\" + 0.010*\"don\"'),\n",
              " (1,\n",
              "  '0.024*\"ground\" + 0.018*\"current\" + 0.018*\"just\" + 0.013*\"want\" + 0.013*\"use\" + 0.011*\"using\" + 0.011*\"used\" + 0.010*\"power\" + 0.010*\"speed\" + 0.010*\"output\"'),\n",
              " (2,\n",
              "  '0.061*\"drive\" + 0.042*\"disk\" + 0.033*\"scsi\" + 0.030*\"drives\" + 0.028*\"hard\" + 0.028*\"controller\" + 0.027*\"card\" + 0.020*\"rom\" + 0.018*\"floppy\" + 0.017*\"bus\"'),\n",
              " (3,\n",
              "  '0.023*\"time\" + 0.015*\"atheism\" + 0.014*\"list\" + 0.013*\"left\" + 0.012*\"alt\" + 0.012*\"faq\" + 0.012*\"probably\" + 0.011*\"know\" + 0.011*\"send\" + 0.010*\"months\"'),\n",
              " (4,\n",
              "  '0.025*\"car\" + 0.016*\"just\" + 0.014*\"don\" + 0.014*\"bike\" + 0.012*\"good\" + 0.011*\"new\" + 0.011*\"think\" + 0.010*\"year\" + 0.010*\"cars\" + 0.010*\"time\"'),\n",
              " (5,\n",
              "  '0.030*\"game\" + 0.027*\"team\" + 0.023*\"year\" + 0.017*\"games\" + 0.016*\"play\" + 0.012*\"season\" + 0.012*\"players\" + 0.012*\"win\" + 0.011*\"hockey\" + 0.011*\"good\"'),\n",
              " (6,\n",
              "  '0.017*\"information\" + 0.014*\"help\" + 0.014*\"medical\" + 0.012*\"new\" + 0.012*\"use\" + 0.012*\"000\" + 0.012*\"research\" + 0.011*\"university\" + 0.010*\"number\" + 0.010*\"program\"'),\n",
              " (7,\n",
              "  '0.022*\"don\" + 0.021*\"people\" + 0.018*\"think\" + 0.017*\"just\" + 0.012*\"say\" + 0.011*\"know\" + 0.011*\"does\" + 0.011*\"good\" + 0.010*\"god\" + 0.009*\"way\"'),\n",
              " (8,\n",
              "  '0.034*\"use\" + 0.023*\"apple\" + 0.020*\"power\" + 0.016*\"time\" + 0.015*\"data\" + 0.015*\"software\" + 0.012*\"pin\" + 0.012*\"memory\" + 0.012*\"simms\" + 0.012*\"port\"'),\n",
              " (9,\n",
              "  '0.068*\"space\" + 0.036*\"nasa\" + 0.021*\"science\" + 0.020*\"edu\" + 0.019*\"data\" + 0.017*\"shuttle\" + 0.015*\"launch\" + 0.015*\"available\" + 0.014*\"center\" + 0.014*\"sci\"')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnmXFkJ9Q4gP",
        "colab_type": "text"
      },
      "source": [
        "#### topic_distribution, given a new document\n",
        "\n",
        "*This function should return a list of tuples (topic_id, probability)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mfV52gjQ4gQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_doc = [\"\\n\\nIt's my understanding that the freezing will start to occur because \\\n",
        "of the\\ngrowing distance of Pluto and Charon from the Sun, due to it's\\nelliptical orbit. \\\n",
        "It is not due to shadowing effects. \\n\\n\\nPluto can shadow Charon, and vice-versa.\\n\\nGeorge \\\n",
        "Krumins\\n-- \"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeJS1zaGQ4gS",
        "colab_type": "code",
        "outputId": "e836b0d7-23d9-41cb-e445-7bf9bbcba283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "def topic_distribution():\n",
        "    # list to sparse matrix, for test doc, only transform\n",
        "    X_test = vect.transform(new_doc)\n",
        "    \n",
        "    # matrix to gensim corpus\n",
        "    corpus = gensim.matutils.Sparse2Corpus(X_test, documents_columns=False)\n",
        "    \n",
        "    # topic_list of type gensim.interfaces.TransformedCorpus\n",
        "    # list of tuples (topic_id, probability) \n",
        "    topic_list = ldamodel.get_document_topics(corpus)\n",
        "    #print(len(topic_list), list(topic_list))\n",
        "    \n",
        "    return list(topic_list)[0]\n",
        "\n",
        "topic_distribution()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0.02000183),\n",
              " (1, 0.020002047),\n",
              " (2, 0.020000001),\n",
              " (3, 0.49658147),\n",
              " (4, 0.020002764),\n",
              " (5, 0.020002853),\n",
              " (6, 0.020001695),\n",
              " (7, 0.020001367),\n",
              " (8, 0.020001847),\n",
              " (9, 0.3434041)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eGN5CyLQ4gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}