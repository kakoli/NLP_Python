{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyR9KbJ_q1Ou"
   },
   "source": [
    "### Analyse text data and create models to predict if a message is spam or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_rf0JFFGTTm"
   },
   "source": [
    "Read csv file and analyse the text data to classify as spam or  not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "l93B1W4TGTTo",
    "outputId": "d1dfd2e0-06c7-46ee-d8de-01a763794f1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "input_data = pd.read_csv('spam.csv')\n",
    "\n",
    "input_data['target'] = np.where(input_data['target']=='spam', 1, 0)\n",
    "input_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xzT-mS0GTT1",
    "outputId": "377fa876-f78b-4fcf-d432-5e3f17f250da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4179, 1393)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data['text'], \n",
    "                                                    input_data['target'], \n",
    "                                                    random_state=0)\n",
    "# X_train of type Series\n",
    "X_train.shape[0], X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2KxmjabqGTT7"
   },
   "source": [
    "Percentage of documents that are spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (747, 2)\n",
      "<class 'pandas.core.series.Series'> (747,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gets entire row based on column 'target' == 1\n",
    "spam_df = input_data[input_data['target'] == 1]\n",
    "print(type(spam_df), spam_df.shape)\n",
    "spam_text = input_data[input_data['target'] == 1]['text']\n",
    "print(type(spam_text), spam_text.shape)\n",
    "\n",
    "spam_cnt = spam_df.shape[0]\n",
    "total_cnt = input_data.shape[0]\n",
    "\n",
    "(spam_cnt*100)/total_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArYV4_YFGTUU"
   },
   "source": [
    "#### Build the model\n",
    "\n",
    "Fit and transform the training data `X_train` using a Count Vectorizer with default parameters.<br>\n",
    "Next, fit a Multinomial Naive Bayes classifier model with smoothing `alpha=0.1`. This classifier is suitable to work with discrete features (e.g., word counts for text classification). <br>\n",
    "Find the area under the curve (AUC) score using the transformed test data. <br>\n",
    "\n",
    "#### Best is MultinomialNB with CountVectorizer. Next is LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVzz-f-5GTUW",
    "outputId": "c034ff05-b023-47de-b75e-0ffbcb7b4519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 7354)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9720812182741116"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "cvec = CountVectorizer().fit(X_train)\n",
    "    \n",
    "nb_clf = MultinomialNB(alpha=0.1)\n",
    "X_train_vec = cvec.transform(X_train)\n",
    "print(X_train_vec.shape)\n",
    "nb_clf.fit(X_train_vec, y_train)\n",
    "    \n",
    "y_pred = nb_clf.predict(cvec.transform(X_test))\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDGSACfNGTUu",
    "outputId": "59c8a287-eb66-46b3-fcba-d00bf5c60569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 7354)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9492385786802031"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfvec = TfidfVectorizer().fit(X_train)\n",
    "\n",
    "nb_clf2 = MultinomialNB(alpha=0.1)\n",
    "nb_clf2.fit(tfvec.transform(X_train), y_train)\n",
    "print(tfvec.transform(X_train).shape)\n",
    "    \n",
    "y_pred = nb_clf2.predict(tfvec.transform(X_test))\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.934010152284264"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = SVC(C=10000).fit(X_train_vec, y_train)\n",
    "y_pred = svm_clf.predict(cvec.transform(X_test))\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9437443763475543"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(C=100).fit(X_train_vec, y_train)\n",
    "y_pred = log_clf.predict(cvec.transform(X_test))\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gvYCcTzdGTUz"
   },
   "source": [
    "From the analysis below it is found that document length, number of digits, number of special characters are much higher for spam documents.\n",
    "#### Average length of documents (number of characters) for non-spam and spam documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Series of the 'text' column for rows of non-spam\n",
    "nonspam_text = input_data[input_data['target'] == 0]['text']\n",
    "non_spam_len = nonspam_text.str.len()\n",
    "non_spam_mean = non_spam_len.mean()\n",
    "\n",
    "spam_text = input_data[input_data['target'] == 1]['text']\n",
    "spam_len = spam_text.str.len()\n",
    "spam_mean = spam_len.mean()\n",
    "\n",
    "(non_spam_mean, spam_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average number of digits for not spam and spam documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2992746113989637, 15.759036144578314)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Series of digit count of each row of text\n",
    "nonspam_digitCnt = nonspam_text.str.count('\\d')\n",
    "\n",
    "spam_digitCnt = spam_text.str.count('\\d')\n",
    "\n",
    "(nonspam_digitCnt.mean(), spam_digitCnt.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam and spam documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.29181347150259, 29.041499330655956)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Series of non-word character count of each row of text\n",
    "nonspam_spCharCnt = nonspam_text.str.count('\\W')\n",
    "\n",
    "spam_spCharCnt = spam_text.str.count('\\W')\n",
    "\n",
    "(nonspam_spCharCnt.mean(), spam_spCharCnt.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qKDofOxGTVa"
   },
   "source": [
    "#### Function to combine new features into the training data\n",
    "Returns sparse feature matrix with added feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X-gXhzzoGTVb"
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# feature_to_add can also be a list of features\n",
    "# return type scipy.sparse.csr.csr_matrix\n",
    "\n",
    "def add_feature(X, feature_to_add):\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXlkPWYOGTVf"
   },
   "source": [
    "Fit and transform the training data X_train using a CountVectorizer ignoring terms that have a document frequency lower than **5**.<br>\n",
    "Using this matrix and two additional features:\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "\n",
    "fit a MultinomialNB.\n",
    "\n",
    "#### Accuracy is 97.09%\n",
    "For default CountVectorizer, number of features are much more and accuracy is 97.2%. Here, with lesser features, but 2 additional meaningful features, accuracy is 97.09%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpjx9dItGTVh",
    "outputId": "7606355f-527e-44ee-f17f-2d338fb2ed26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 1470)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9708567475340815"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer(min_df=5).fit(X_train)\n",
    "X_train_vec = cvec.transform(X_train)     \n",
    "   \n",
    "# shape inc from 4179*1468 -> 4179*1470\n",
    "# parameters of add_feature are Series or list of Series, return type scipy.sparse.csr.csr_matrix\n",
    "new_train_features = [X_train.str.len(), X_train.str.count('\\d')]\n",
    "X_train_vec_new = add_feature(X_train_vec, new_train_features)\n",
    "print(X_train_vec_new.shape)\n",
    "nb_clf = MultinomialNB(alpha=0.1).fit(X_train_vec_new, y_train)\n",
    "\n",
    "new_test_features = [X_test.str.len(), X_test.str.count('\\d')]\n",
    "X_test_vec_new = add_feature(cvec.transform(X_test), new_test_features)\n",
    "y_pred = nb_clf.predict(X_test_vec_new)\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and transform the training data X_train using a Count Vectorizer ignoring terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.** Character n-grams pass in `analyzer='char_wb'` which creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n",
    "\n",
    "Using this matrix and three additional features:\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* number of non-word characters\n",
    "\n",
    "#### Accuracy is 98.18%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without document freq threshold, features 7354. With min_df=5, it reduced to 1468. \n",
    "With word ngram(1,3), it increased to 3383. With char ngram(2,5), it increased drastically to 16314.\n",
    "And accuracy increased by more than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpjx9dItGTVh",
    "outputId": "7606355f-527e-44ee-f17f-2d338fb2ed26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 16317)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9818451521993787"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvec = CountVectorizer(min_df=5, ngram_range=(2,5), analyzer='char_wb').fit(X_train)\n",
    "X_train_vec = cvec.transform(X_train)     \n",
    "   \n",
    "# shape inc from 4179*1468 -> 4179*16317\n",
    "new_train_features = [X_train.str.len(), X_train.str.count('\\d'), X_train.str.count('\\W')]\n",
    "X_train_vec_new = add_feature(X_train_vec, new_train_features)\n",
    "print(X_train_vec_new.shape)\n",
    "nb_clf = MultinomialNB(alpha=0.1).fit(X_train_vec_new, y_train)\n",
    "\n",
    "new_test_features = [X_test.str.len(), X_test.str.count('\\d'), X_test.str.count('\\W')]\n",
    "X_test_vec_new = add_feature(cvec.transform(X_test), new_test_features)\n",
    "y_pred = nb_clf.predict(X_test_vec_new)\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8vfKYp9GTV0"
   },
   "source": [
    "Find the 10 smallest and 10 largest coefficients from the model.\n",
    "The list of 10 smallest coefficients should be sorted smallest first, the list of 10 largest coefficients should be sorted largest first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 feature names [' !' ' ! ' ' !!' ' !! ' \" !!'\" \" !!''\" ' #' ' $' ' $ ' ' &']\n",
      "Last 10 feature names ['non_word_char_count' 'digit_count' 'length_of_doc' 'û÷t ' 'û÷t' 'û÷m '\n",
      " 'û÷m' 'û÷ll ' 'û÷ll' 'û÷l']\n",
      "\n",
      "Sorted features :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['mount',\n",
       "  'aptop',\n",
       "  'apto',\n",
       "  'apt',\n",
       "  'april',\n",
       "  'apri',\n",
       "  'apr',\n",
       "  'appy.',\n",
       "  'appy ',\n",
       "  'appy'],\n",
       " ['length_of_doc',\n",
       "  'non_word_char_count',\n",
       "  'digit_count',\n",
       "  'e ',\n",
       "  ' t',\n",
       "  ' c',\n",
       "  't ',\n",
       "  's ',\n",
       "  'r ',\n",
       "  'to'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model coef(weights) are the same for each data point, got from training.\n",
    "# Features are what defines the data points\n",
    "feature_list = cvec.get_feature_names()\n",
    "\n",
    "# Feature list is increased to 16317 to match the matrix\n",
    "feature_list.extend(['length_of_doc', 'digit_count', 'non_word_char_count'])\n",
    "# List converted to array so that indexing can be done with another array\n",
    "feature_array = np.array(feature_list)\n",
    "\n",
    "# Printing unsorted feature names \n",
    "print(\"First 10 feature names\", feature_array[:10])\n",
    "print(\"Last 10 feature names\", feature_array[:-11:-1])\n",
    "\n",
    "sorted_coef_index = nb_clf.coef_[0].argsort()\n",
    "#print(\"Indices of the largest coef \", sorted_coef_index[:-11:-1])\n",
    "\n",
    "small_features = feature_array[sorted_coef_index[:10]]\n",
    "large_features = feature_array[sorted_coef_index[:-11:-1]]\n",
    "\n",
    "print(\"\\nSorted features :\")\n",
    "small_features.tolist(), large_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxLiJUuYGTV9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SpamClassifier.ipynb",
   "provenance": []
  },
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
